import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

def preprocess_text(text):
    # B∆∞·ªõc 1: L√†m s·∫°ch
    text = text.replace("\n", " ")
    text = re.sub(r"[i|r|k|d]\)", "-", text)
    text = re.sub(r"ng\.\.\.", "v.v.", text)
    text = " ".join(text.split())

    # B∆∞·ªõc 2: T√°i c·∫•u tr√∫c
    text = text.replace("1.", "1. ").replace("2.", "2. ")
    text = text.replace(";", "; ").replace("-", "- ")

    return text



# -----------------------------------
# C·∫•u h√¨nh ·ª©ng d·ª•ng
# -----------------------------------

# T√™n m√¥ h√¨nh tr√™n HuggingFace
HF_MODEL_NAME = "Lusic/testmodel"

# C√°c tham s·ªë c·∫•u h√¨nh cho vi·ªác sinh vƒÉn b·∫£n
MAX_NEW_TOKENS = 256  # S·ªë l∆∞·ª£ng token t·ªëi ƒëa sinh ra
TEMPERATURE = 1.5  # ƒê·ªô ng·∫´u nhi√™n trong vi·ªác sinh vƒÉn b·∫£n
TOP_P = 0.9  # Gi√° tr·ªã x√°c su·∫•t t√≠ch l≈©y (top-p sampling)

# Ki·ªÉm tra v√† ch·ªçn thi·∫øt b·ªã (GPU n·∫øu c√≥, n·∫øu kh√¥ng s·ª≠ d·ª•ng CPU)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------------
# Kh·ªüi t·∫°o m√¥ h√¨nh v√† tokenizer
# -----------------------------------

@st.cache_resource
def initialize_model_and_tokenizer(model_name):
    """
    H√†m n√†y t·∫£i m√¥ h√¨nh v√† tokenizer t·ª´ HuggingFace.
    Sau khi t·∫£i, m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ƒë·∫øn thi·∫øt b·ªã t∆∞∆°ng ·ª©ng (GPU ho·∫∑c CPU).
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        model.to(DEVICE)
        model.eval()
        return tokenizer, model

    except Exception as e:
        # Th√¥ng b√°o l·ªói n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c m√¥ h√¨nh ho·∫∑c tokenizer
        st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh ho·∫∑c tokenizer: {e}")
        st.stop()

# Kh·ªüi t·∫°o tokenizer v√† model
tokenizer, model = initialize_model_and_tokenizer(HF_MODEL_NAME='Lusic/testmodel')

# -----------------------------------
# Giao di·ªán Streamlit
# -----------------------------------

# Ti√™u ƒë·ªÅ c·ªßa ·ª©ng d·ª•ng
st.title("ü§ñ ·ª®ng D·ª•ng Chat v·ªõi M√¥ H√¨nh Ng√¥n Ng·ªØ")

# M√¥ t·∫£ ·ª©ng d·ª•ng
st.markdown(
    """
    Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi ·ª©ng d·ª•ng Chat AI! ·ª®ng d·ª•ng n√†y cho ph√©p b·∫°n t∆∞∆°ng t√°c v·ªõi
    m√¥ h√¨nh ng√¥n ng·ªØ **Lusic/testmodel** t·ª´ HuggingFace. H√£y nh·∫≠p tin nh·∫Øn b√™n d∆∞·ªõi v√† nh·∫≠n ph·∫£n h·ªìi t·ª´ AI.
    """
)

# Nh·∫≠p tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
user_input = st.text_area("üìù Tin nh·∫Øn c·ªßa b·∫°n:", height=150, placeholder="Nh·∫≠p n·ªôi dung tin nh·∫Øn t·∫°i ƒë√¢y...")
if st.button("üí¨ T·∫°o ph·∫£n h·ªìi"):
    if user_input.strip() == "":  # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng ch∆∞a nh·∫≠p n·ªôi dung
        st.warning("Vui l√≤ng nh·∫≠p tin nh·∫Øn ƒë·ªÉ nh·∫≠n ph·∫£n h·ªìi.")
    else:
        with st.spinner("üîÑ ƒêang sinh ph·∫£n h·ªìi..."):
            try:
                messages = [
                    {"role": "user", "content": user_input}
                ]

                # Chuy·ªÉn tin nh·∫Øn th√†nh input cho m√¥ h√¨nh (d√πng tokenizer)
                inputs = tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,  
                    return_tensors="pt",
                ).to(DEVICE)

                # Sinh ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh
                outputs = model.generate(
                    input_ids=inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    use_cache=True,
                    temperature=TEMPERATURE,
                    top_p=TOP_P,
                )

                response = tokenizer.batch_decode(outputs, skip_special_tokens=True)

                st.success("üó®Ô∏è **Ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh:**")
                st.write(preprocess_text(response[0]))

            except Exception as e:
                st.error(f"L·ªói khi t·∫°o ph·∫£n h·ªìi: {e}")

# -----------------------------------
# Sidebar v·ªõi h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
# -----------------------------------

# Sidebar ch·ª©a th√¥ng tin h∆∞·ªõng d·∫´n
st.sidebar.header("üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
st.sidebar.info(
    """
- **Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n:** G√µ n·ªôi dung tin nh·∫Øn v√†o √¥ vƒÉn b·∫£n.
- **T·∫°o ph·∫£n h·ªìi:** Nh·∫•n n√∫t "T·∫°o ph·∫£n h·ªìi" ƒë·ªÉ nh·∫≠n c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh.
- **Chi ti·∫øt m√¥ h√¨nh:** ·ª®ng d·ª•ng s·ª≠ d·ª•ng m√¥ h√¨nh HuggingFace [Lusic/testmodel](https://huggingface.co/Lusic/testmodel).
- **Y√™u c·∫ßu h·ªá th·ªëng:** Khuy·∫øn ngh·ªã ch·∫°y tr√™n m√°y t√≠nh c√≥ GPU ƒë·ªÉ ƒë·∫°t hi·ªáu su·∫•t cao nh·∫•t.
- **L∆∞u √Ω:** ƒê·∫£m b·∫£o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.
    """
)

# -----------------------------------
# Footer
# -----------------------------------

# Ph·∫ßn ghi ch√∫ d∆∞·ªõi c√πng c·ªßa ·ª©ng d·ª•ng
st.markdown(
    """
    ---
    üõ†Ô∏è **Ph√°t tri·ªÉn b·ªüi Streamlit** | üìÖ **Ng√†y:** 8 Th√°ng 12, 2024
    """
)
